/opt/conda/lib/python3.10/site-packages/torch/distributed/launch.py:180: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use_env is set by default in torchrun.
If your script expects `--local_rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
| distributed init (rank 1): env://, gpu 1
| distributed init (rank 2): env://, gpu 2
| distributed init (rank 3): env://, gpu 3
| distributed init (rank 0): env://, gpu 0
[10:19:10.410907] Namespace(batch_size=128, epochs=100, accum_iter=2, model='vit_base_patch16', input_size=224, drop_path=0.1, clip_grad=None, weight_decay=0.05, lr=0.004, blr=0.001, layer_decay=0.75, min_lr=1e-06, warmup_epochs=20, color_jitter=None, aa='rand-m9-mstd0.5-inc1', smoothing=0.1, reprob=0.25, remode='pixel', recount=1, resplit=False, mixup=0.8, cutmix=1.0, cutmix_minmax=None, mixup_prob=1.0, mixup_switch_prob=0.5, mixup_mode='batch', finetune='/raid/home_yedek/utku/freezeout_localmim_rho/ViT/full_pretrain_out_freezeout/checkpoint-99.pth', global_pool=True, data_path='/raid/utku/datasets/imagenet/classification/train/image_folders', nb_classes=1000, output_dir='full_finetune_out_freezeout/', log_dir='full_finetune_out_freezeout', device='cuda', seed=0, resume='', auto_resume=True, start_epoch=0, eval=False, dist_eval=True, num_workers=10, pin_mem=True, world_size=4, local_rank=0, dist_on_itp=False, dist_url='env://', rank=0, gpu=0, distributed=True, dist_backend='nccl')
[10:19:15.924571] Dataset ImageFolder
    Number of datapoints: 1281167
    Root location: /raid/utku/datasets/imagenet/classification/train/image_folders
    StandardTransform
Transform: Compose(
               RandomResizedCropAndInterpolation(size=(224, 224), scale=(0.08, 1.0), ratio=(0.75, 1.3333), interpolation=bicubic)
               RandomHorizontalFlip(p=0.5)
               RandAugment(n=2, ops=
           	AugmentOp(name=AutoContrast, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=Equalize, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=Invert, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=Rotate, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=PosterizeIncreasing, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=SolarizeIncreasing, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=SolarizeAdd, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=ColorIncreasing, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=ContrastIncreasing, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=BrightnessIncreasing, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=SharpnessIncreasing, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=ShearX, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=ShearY, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=TranslateXRel, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=TranslateYRel, p=0.5, m=9, mstd=0.5))
               ToTensor()
               Normalize(mean=tensor([0.4850, 0.4560, 0.4060]), std=tensor([0.2290, 0.2240, 0.2250]))
               RandomErasing(p=0.25, mode=pixel, count=(1, 1))
           )
[10:19:21.383326] Dataset ImageFolder
    Number of datapoints: 1281167
    Root location: /raid/utku/datasets/imagenet/classification/train/image_folders
    StandardTransform
Transform: Compose(
               Resize(size=256, interpolation=bicubic, max_size=None, antialias=None)
               CenterCrop(size=(224, 224))
               ToTensor()
               Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))
           )
[10:19:21.383628] Sampler_train = <torch.utils.data.distributed.DistributedSampler object at 0x7f08f51df6d0>
[10:19:21.383659] Warning: Enabling distributed evaluation with an eval dataset not divisible by process number. This will slightly alter validation results as extra duplicate entries are added to achieve equal num of samples per-process.
[10:19:21.386995] Mixup is activated!
[10:19:23.336578] Load pre-trained checkpoint from: /raid/home_yedek/utku/freezeout_localmim_rho/ViT/full_pretrain_out_freezeout/checkpoint-99.pth
[10:19:23.416174] _IncompatibleKeys(missing_keys=['fc_norm.weight', 'fc_norm.bias', 'head.weight', 'head.bias'], unexpected_keys=['norm.0.weight', 'norm.0.bias', 'norm.1.weight', 'norm.1.bias', 'norm.2.weight', 'norm.2.bias', 'norm.3.weight', 'norm.3.bias', 'decoder.0.mask_token', 'decoder.0.pos_embed', 'decoder.0.embed.weight', 'decoder.0.embed.bias', 'decoder.0.blocks.0.norm1.weight', 'decoder.0.blocks.0.norm1.bias', 'decoder.0.blocks.0.attn.qkv.weight', 'decoder.0.blocks.0.attn.qkv.bias', 'decoder.0.blocks.0.attn.proj.weight', 'decoder.0.blocks.0.attn.proj.bias', 'decoder.0.blocks.0.norm2.weight', 'decoder.0.blocks.0.norm2.bias', 'decoder.0.blocks.0.mlp.fc1.weight', 'decoder.0.blocks.0.mlp.fc1.bias', 'decoder.0.blocks.0.mlp.fc2.weight', 'decoder.0.blocks.0.mlp.fc2.bias', 'decoder.0.norm.weight', 'decoder.0.norm.bias', 'decoder.0.pred.0.weight', 'decoder.0.pred.0.bias', 'decoder.0.pred.1.weight', 'decoder.0.pred.1.bias', 'decoder.0.pred.3.weight', 'decoder.0.pred.3.bias', 'decoder.0.pred.4.weight', 'decoder.0.pred.4.bias', 'decoder.1.mask_token', 'decoder.1.pos_embed', 'decoder.1.embed.weight', 'decoder.1.embed.bias', 'decoder.1.blocks.0.norm1.weight', 'decoder.1.blocks.0.norm1.bias', 'decoder.1.blocks.0.attn.qkv.weight', 'decoder.1.blocks.0.attn.qkv.bias', 'decoder.1.blocks.0.attn.proj.weight', 'decoder.1.blocks.0.attn.proj.bias', 'decoder.1.blocks.0.norm2.weight', 'decoder.1.blocks.0.norm2.bias', 'decoder.1.blocks.0.mlp.fc1.weight', 'decoder.1.blocks.0.mlp.fc1.bias', 'decoder.1.blocks.0.mlp.fc2.weight', 'decoder.1.blocks.0.mlp.fc2.bias', 'decoder.1.norm.weight', 'decoder.1.norm.bias', 'decoder.1.pred.0.weight', 'decoder.1.pred.0.bias', 'decoder.1.pred.1.weight', 'decoder.1.pred.1.bias', 'decoder.2.mask_token', 'decoder.2.pos_embed', 'decoder.2.embed.weight', 'decoder.2.embed.bias', 'decoder.2.blocks.0.norm1.weight', 'decoder.2.blocks.0.norm1.bias', 'decoder.2.blocks.0.attn.qkv.weight', 'decoder.2.blocks.0.attn.qkv.bias', 'decoder.2.blocks.0.attn.proj.weight', 'decoder.2.blocks.0.attn.proj.bias', 'decoder.2.blocks.0.norm2.weight', 'decoder.2.blocks.0.norm2.bias', 'decoder.2.blocks.0.mlp.fc1.weight', 'decoder.2.blocks.0.mlp.fc1.bias', 'decoder.2.blocks.0.mlp.fc2.weight', 'decoder.2.blocks.0.mlp.fc2.bias', 'decoder.2.norm.weight', 'decoder.2.norm.bias', 'decoder.2.pred.0.weight', 'decoder.2.pred.0.bias', 'decoder.3.mask_token', 'decoder.3.pos_embed', 'decoder.3.embed.weight', 'decoder.3.embed.bias', 'decoder.3.blocks.0.norm1.weight', 'decoder.3.blocks.0.norm1.bias', 'decoder.3.blocks.0.attn.qkv.weight', 'decoder.3.blocks.0.attn.qkv.bias', 'decoder.3.blocks.0.attn.proj.weight', 'decoder.3.blocks.0.attn.proj.bias', 'decoder.3.blocks.0.norm2.weight', 'decoder.3.blocks.0.norm2.bias', 'decoder.3.blocks.0.mlp.fc1.weight', 'decoder.3.blocks.0.mlp.fc1.bias', 'decoder.3.blocks.0.mlp.fc2.weight', 'decoder.3.blocks.0.mlp.fc2.bias', 'decoder.3.norm.weight', 'decoder.3.norm.bias', 'decoder.3.pred.1.weight', 'decoder.3.pred.1.bias', 'hog_enc.0.conv.weight', 'hog_enc.1.conv.weight', 'hog_enc.2.conv.weight', 'hog_enc.3.conv.weight'])
[10:19:23.491955] number of params (M): 86.57
[10:19:23.492033] base lr: 1.00e-03
[10:19:23.492050] actual lr: 4.00e-03
[10:19:23.492083] accumulate grad iterations: 2
[10:19:23.492097] effective batch size: 1024
[10:19:23.577020] parameter groups: 
{
  "layer_0_no_decay": {
    "lr_scale": 0.023757264018058777,
    "weight_decay": 0.0,
    "params": [
      "cls_token",
      "pos_embed",
      "patch_embed.proj.bias"
    ]
  },
  "layer_0_decay": {
    "lr_scale": 0.023757264018058777,
    "weight_decay": 0.05,
    "params": [
      "patch_embed.proj.weight"
    ]
  },
  "layer_1_no_decay": {
    "lr_scale": 0.03167635202407837,
    "weight_decay": 0.0,
    "params": [
      "blocks.0.norm1.weight",
      "blocks.0.norm1.bias",
      "blocks.0.attn.qkv.bias",
      "blocks.0.attn.proj.bias",
      "blocks.0.norm2.weight",
      "blocks.0.norm2.bias",
      "blocks.0.mlp.fc1.bias",
      "blocks.0.mlp.fc2.bias"
    ]
  },
  "layer_1_decay": {
    "lr_scale": 0.03167635202407837,
    "weight_decay": 0.05,
    "params": [
      "blocks.0.attn.qkv.weight",
      "blocks.0.attn.proj.weight",
      "blocks.0.mlp.fc1.weight",
      "blocks.0.mlp.fc2.weight"
    ]
  },
  "layer_2_no_decay": {
    "lr_scale": 0.04223513603210449,
    "weight_decay": 0.0,
    "params": [
      "blocks.1.norm1.weight",
      "blocks.1.norm1.bias",
      "blocks.1.attn.qkv.bias",
      "blocks.1.attn.proj.bias",
      "blocks.1.norm2.weight",
      "blocks.1.norm2.bias",
      "blocks.1.mlp.fc1.bias",
      "blocks.1.mlp.fc2.bias"
    ]
  },
  "layer_2_decay": {
    "lr_scale": 0.04223513603210449,
    "weight_decay": 0.05,
    "params": [
      "blocks.1.attn.qkv.weight",
      "blocks.1.attn.proj.weight",
      "blocks.1.mlp.fc1.weight",
      "blocks.1.mlp.fc2.weight"
    ]
  },
  "layer_3_no_decay": {
    "lr_scale": 0.056313514709472656,
    "weight_decay": 0.0,
    "params": [
      "blocks.2.norm1.weight",
      "blocks.2.norm1.bias",
      "blocks.2.attn.qkv.bias",
      "blocks.2.attn.proj.bias",
      "blocks.2.norm2.weight",
      "blocks.2.norm2.bias",
      "blocks.2.mlp.fc1.bias",
      "blocks.2.mlp.fc2.bias"
    ]
  },
  "layer_3_decay": {
    "lr_scale": 0.056313514709472656,
    "weight_decay": 0.05,
    "params": [
      "blocks.2.attn.qkv.weight",
      "blocks.2.attn.proj.weight",
      "blocks.2.mlp.fc1.weight",
      "blocks.2.mlp.fc2.weight"
    ]
  },
  "layer_4_no_decay": {
    "lr_scale": 0.07508468627929688,
    "weight_decay": 0.0,
    "params": [
      "blocks.3.norm1.weight",
      "blocks.3.norm1.bias",
      "blocks.3.attn.qkv.bias",
      "blocks.3.attn.proj.bias",
      "blocks.3.norm2.weight",
      "blocks.3.norm2.bias",
      "blocks.3.mlp.fc1.bias",
      "blocks.3.mlp.fc2.bias"
    ]
  },
  "layer_4_decay": {
    "lr_scale": 0.07508468627929688,
    "weight_decay": 0.05,
    "params": [
      "blocks.3.attn.qkv.weight",
      "blocks.3.attn.proj.weight",
      "blocks.3.mlp.fc1.weight",
      "blocks.3.mlp.fc2.weight"
    ]
  },
  "layer_5_no_decay": {
    "lr_scale": 0.1001129150390625,
    "weight_decay": 0.0,
    "params": [
      "blocks.4.norm1.weight",
      "blocks.4.norm1.bias",
      "blocks.4.attn.qkv.bias",
      "blocks.4.attn.proj.bias",
      "blocks.4.norm2.weight",
      "blocks.4.norm2.bias",
      "blocks.4.mlp.fc1.bias",
      "blocks.4.mlp.fc2.bias"
    ]
  },
  "layer_5_decay": {
    "lr_scale": 0.1001129150390625,
    "weight_decay": 0.05,
    "params": [
      "blocks.4.attn.qkv.weight",
      "blocks.4.attn.proj.weight",
      "blocks.4.mlp.fc1.weight",
      "blocks.4.mlp.fc2.weight"
    ]
  },
  "layer_6_no_decay": {
    "lr_scale": 0.13348388671875,
    "weight_decay": 0.0,
    "params": [
      "blocks.5.norm1.weight",
      "blocks.5.norm1.bias",
      "blocks.5.attn.qkv.bias",
      "blocks.5.attn.proj.bias",
      "blocks.5.norm2.weight",
      "blocks.5.norm2.bias",
      "blocks.5.mlp.fc1.bias",
      "blocks.5.mlp.fc2.bias"
    ]
  },
  "layer_6_decay": {
    "lr_scale": 0.13348388671875,
    "weight_decay": 0.05,
    "params": [
      "blocks.5.attn.qkv.weight",
      "blocks.5.attn.proj.weight",
      "blocks.5.mlp.fc1.weight",
      "blocks.5.mlp.fc2.weight"
    ]
  },
  "layer_7_no_decay": {
    "lr_scale": 0.177978515625,
    "weight_decay": 0.0,
    "params": [
      "blocks.6.norm1.weight",
      "blocks.6.norm1.bias",
      "blocks.6.attn.qkv.bias",
      "blocks.6.attn.proj.bias",
      "blocks.6.norm2.weight",
      "blocks.6.norm2.bias",
      "blocks.6.mlp.fc1.bias",
      "blocks.6.mlp.fc2.bias"
    ]
  },
  "layer_7_decay": {
    "lr_scale": 0.177978515625,
    "weight_decay": 0.05,
    "params": [
      "blocks.6.attn.qkv.weight",
      "blocks.6.attn.proj.weight",
      "blocks.6.mlp.fc1.weight",
      "blocks.6.mlp.fc2.weight"
    ]
  },
  "layer_8_no_decay": {
    "lr_scale": 0.2373046875,
    "weight_decay": 0.0,
    "params": [
      "blocks.7.norm1.weight",
      "blocks.7.norm1.bias",
      "blocks.7.attn.qkv.bias",
      "blocks.7.attn.proj.bias",
      "blocks.7.norm2.weight",
      "blocks.7.norm2.bias",
      "blocks.7.mlp.fc1.bias",
      "blocks.7.mlp.fc2.bias"
    ]
  },
  "layer_8_decay": {
    "lr_scale": 0.2373046875,
    "weight_decay": 0.05,
    "params": [
      "blocks.7.attn.qkv.weight",
      "blocks.7.attn.proj.weight",
      "blocks.7.mlp.fc1.weight",
      "blocks.7.mlp.fc2.weight"
    ]
  },
  "layer_9_no_decay": {
    "lr_scale": 0.31640625,
    "weight_decay": 0.0,
    "params": [
      "blocks.8.norm1.weight",
      "blocks.8.norm1.bias",
      "blocks.8.attn.qkv.bias",
      "blocks.8.attn.proj.bias",
      "blocks.8.norm2.weight",
      "blocks.8.norm2.bias",
      "blocks.8.mlp.fc1.bias",
      "blocks.8.mlp.fc2.bias"
    ]
  },
  "layer_9_decay": {
    "lr_scale": 0.31640625,
    "weight_decay": 0.05,
    "params": [
      "blocks.8.attn.qkv.weight",
      "blocks.8.attn.proj.weight",
      "blocks.8.mlp.fc1.weight",
      "blocks.8.mlp.fc2.weight"
    ]
  },
  "layer_10_no_decay": {
    "lr_scale": 0.421875,
    "weight_decay": 0.0,
    "params": [
      "blocks.9.norm1.weight",
      "blocks.9.norm1.bias",
      "blocks.9.attn.qkv.bias",
      "blocks.9.attn.proj.bias",
      "blocks.9.norm2.weight",
      "blocks.9.norm2.bias",
      "blocks.9.mlp.fc1.bias",
      "blocks.9.mlp.fc2.bias"
    ]
  },
  "layer_10_decay": {
    "lr_scale": 0.421875,
    "weight_decay": 0.05,
    "params": [
      "blocks.9.attn.qkv.weight",
      "blocks.9.attn.proj.weight",
      "blocks.9.mlp.fc1.weight",
      "blocks.9.mlp.fc2.weight"
    ]
  },
  "layer_11_no_decay": {
    "lr_scale": 0.5625,
    "weight_decay": 0.0,
    "params": [
      "blocks.10.norm1.weight",
      "blocks.10.norm1.bias",
      "blocks.10.attn.qkv.bias",
      "blocks.10.attn.proj.bias",
      "blocks.10.norm2.weight",
      "blocks.10.norm2.bias",
      "blocks.10.mlp.fc1.bias",
      "blocks.10.mlp.fc2.bias"
    ]
  },
  "layer_11_decay": {
    "lr_scale": 0.5625,
    "weight_decay": 0.05,
    "params": [
      "blocks.10.attn.qkv.weight",
      "blocks.10.attn.proj.weight",
      "blocks.10.mlp.fc1.weight",
      "blocks.10.mlp.fc2.weight"
    ]
  },
  "layer_12_no_decay": {
    "lr_scale": 0.75,
    "weight_decay": 0.0,
    "params": [
      "blocks.11.norm1.weight",
      "blocks.11.norm1.bias",
      "blocks.11.attn.qkv.bias",
      "blocks.11.attn.proj.bias",
      "blocks.11.norm2.weight",
      "blocks.11.norm2.bias",
      "blocks.11.mlp.fc1.bias",
      "blocks.11.mlp.fc2.bias"
    ]
  },
  "layer_12_decay": {
    "lr_scale": 0.75,
    "weight_decay": 0.05,
    "params": [
      "blocks.11.attn.qkv.weight",
      "blocks.11.attn.proj.weight",
      "blocks.11.mlp.fc1.weight",
      "blocks.11.mlp.fc2.weight"
    ]
  },
  "layer_13_no_decay": {
    "lr_scale": 1.0,
    "weight_decay": 0.0,
    "params": [
      "fc_norm.weight",
      "fc_norm.bias",
      "head.bias"
    ]
  },
  "layer_13_decay": {
    "lr_scale": 1.0,
    "weight_decay": 0.05,
    "params": [
      "head.weight"
    ]
  }
}
[10:19:23.578263] criterion = SoftTargetCrossEntropy()
[10:19:23.579085] Auto resume checkpoint: 
[10:19:23.579135] Start training for 100 epochs
[10:19:23.581072] log_dir: full_finetune_out_freezeout
[10:19:30.834275] Epoch: [0]  [   0/2502]  eta: 5:02:24  lr: 0.000000  loss: 6.9078 (6.9078)  time: 7.2521  data: 3.1895  max mem: 13378
[10:19:50.238918] Epoch: [0]  [  50/2502]  eta: 0:21:21  lr: 0.000004  loss: 6.9077 (6.9077)  time: 0.3878  data: 0.0006  max mem: 14369
[10:20:09.664985] Epoch: [0]  [ 100/2502]  eta: 0:18:15  lr: 0.000008  loss: 6.9074 (6.9077)  time: 0.3893  data: 0.0006  max mem: 14369
[10:20:29.162055] Epoch: [0]  [ 150/2502]  eta: 0:17:01  lr: 0.000012  loss: 6.9072 (6.9075)  time: 0.3896  data: 0.0006  max mem: 14369
[10:20:48.643981] Epoch: [0]  [ 200/2502]  eta: 0:16:14  lr: 0.000016  loss: 6.9061 (6.9073)  time: 0.3895  data: 0.0006  max mem: 14369
[10:21:08.182053] Epoch: [0]  [ 250/2502]  eta: 0:15:38  lr: 0.000020  loss: 6.9053 (6.9070)  time: 0.3899  data: 0.0006  max mem: 14369
[10:21:27.730572] Epoch: [0]  [ 300/2502]  eta: 0:15:08  lr: 0.000024  loss: 6.9030 (6.9066)  time: 0.3906  data: 0.0006  max mem: 14369
[10:21:47.239350] Epoch: [0]  [ 350/2502]  eta: 0:14:40  lr: 0.000028  loss: 6.9005 (6.9058)  time: 0.3888  data: 0.0006  max mem: 14369
[10:22:06.855043] Epoch: [0]  [ 400/2502]  eta: 0:14:15  lr: 0.000032  loss: 6.8761 (6.9030)  time: 0.3914  data: 0.0006  max mem: 14369
[10:22:26.396649] Epoch: [0]  [ 450/2502]  eta: 0:13:51  lr: 0.000036  loss: 6.8253 (6.8965)  time: 0.3901  data: 0.0005  max mem: 14369
[10:22:46.032603] Epoch: [0]  [ 500/2502]  eta: 0:13:28  lr: 0.000040  loss: 6.7808 (6.8852)  time: 0.3915  data: 0.0006  max mem: 14369
[10:23:05.638663] Epoch: [0]  [ 550/2502]  eta: 0:13:06  lr: 0.000044  loss: 6.7347 (6.8719)  time: 0.3919  data: 0.0006  max mem: 14369
[10:23:25.355029] Epoch: [0]  [ 600/2502]  eta: 0:12:45  lr: 0.000048  loss: 6.7038 (6.8600)  time: 0.3946  data: 0.0006  max mem: 14369
[10:23:44.993387] Epoch: [0]  [ 650/2502]  eta: 0:12:23  lr: 0.000052  loss: 6.6610 (6.8468)  time: 0.3920  data: 0.0006  max mem: 14369
[10:24:04.714365] Epoch: [0]  [ 700/2502]  eta: 0:12:02  lr: 0.000056  loss: 6.6054 (6.8326)  time: 0.3930  data: 0.0006  max mem: 14369
[10:24:24.335727] Epoch: [0]  [ 750/2502]  eta: 0:11:41  lr: 0.000060  loss: 6.6738 (6.8194)  time: 0.3935  data: 0.0006  max mem: 14369
[10:24:43.921817] Epoch: [0]  [ 800/2502]  eta: 0:11:20  lr: 0.000064  loss: 6.6031 (6.8064)  time: 0.3920  data: 0.0006  max mem: 14369
[10:25:03.561907] Epoch: [0]  [ 850/2502]  eta: 0:10:59  lr: 0.000068  loss: 6.5724 (6.7907)  time: 0.3909  data: 0.0006  max mem: 14369
[10:25:23.243740] Epoch: [0]  [ 900/2502]  eta: 0:10:39  lr: 0.000072  loss: 6.5470 (6.7757)  time: 0.3944  data: 0.0006  max mem: 14369
[10:25:42.855439] Epoch: [0]  [ 950/2502]  eta: 0:10:18  lr: 0.000076  loss: 6.3674 (6.7612)  time: 0.3914  data: 0.0006  max mem: 14369
[10:26:02.528026] Epoch: [0]  [1000/2502]  eta: 0:09:58  lr: 0.000080  loss: 6.5247 (6.7476)  time: 0.3935  data: 0.0006  max mem: 14369
[10:26:22.185255] Epoch: [0]  [1050/2502]  eta: 0:09:38  lr: 0.000084  loss: 6.4894 (6.7330)  time: 0.3926  data: 0.0006  max mem: 14369
[10:26:41.828578] Epoch: [0]  [1100/2502]  eta: 0:09:17  lr: 0.000088  loss: 6.3960 (6.7178)  time: 0.3930  data: 0.0006  max mem: 14369
[10:27:01.438520] Epoch: [0]  [1150/2502]  eta: 0:08:57  lr: 0.000092  loss: 6.3896 (6.7046)  time: 0.3927  data: 0.0007  max mem: 14369
[10:27:21.105612] Epoch: [0]  [1200/2502]  eta: 0:08:37  lr: 0.000096  loss: 6.3828 (6.6919)  time: 0.3919  data: 0.0007  max mem: 14369
[10:27:40.721077] Epoch: [0]  [1250/2502]  eta: 0:08:17  lr: 0.000100  loss: 6.2723 (6.6762)  time: 0.3940  data: 0.0006  max mem: 14369
[10:28:00.302211] Epoch: [0]  [1300/2502]  eta: 0:07:57  lr: 0.000104  loss: 6.3623 (6.6639)  time: 0.3918  data: 0.0005  max mem: 14369
[10:28:19.954577] Epoch: [0]  [1350/2502]  eta: 0:07:37  lr: 0.000108  loss: 6.3311 (6.6489)  time: 0.3931  data: 0.0006  max mem: 14369
[10:28:39.561666] Epoch: [0]  [1400/2502]  eta: 0:07:17  lr: 0.000112  loss: 6.3100 (6.6369)  time: 0.3919  data: 0.0006  max mem: 14369
[10:28:59.246598] Epoch: [0]  [1450/2502]  eta: 0:06:57  lr: 0.000116  loss: 6.2659 (6.6245)  time: 0.3948  data: 0.0006  max mem: 14369
[10:29:18.894544] Epoch: [0]  [1500/2502]  eta: 0:06:37  lr: 0.000120  loss: 6.1113 (6.6096)  time: 0.3937  data: 0.0006  max mem: 14369
[10:29:38.517209] Epoch: [0]  [1550/2502]  eta: 0:06:17  lr: 0.000124  loss: 6.1823 (6.5972)  time: 0.3914  data: 0.0007  max mem: 14369
[10:29:58.177179] Epoch: [0]  [1600/2502]  eta: 0:05:57  lr: 0.000128  loss: 6.2399 (6.5842)  time: 0.3928  data: 0.0006  max mem: 14369
[10:30:17.806334] Epoch: [0]  [1650/2502]  eta: 0:05:37  lr: 0.000132  loss: 6.1358 (6.5702)  time: 0.3932  data: 0.0006  max mem: 14369
[10:30:37.441680] Epoch: [0]  [1700/2502]  eta: 0:05:17  lr: 0.000136  loss: 6.2639 (6.5568)  time: 0.3932  data: 0.0006  max mem: 14369
