/opt/conda/lib/python3.10/site-packages/torch/distributed/launch.py:180: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use_env is set by default in torchrun.
If your script expects `--local_rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
| distributed init (rank 0): env://, gpu 0
| distributed init (rank 1): env://, gpu 1
| distributed init (rank 2): env://, gpu 2
| distributed init (rank 3): env://, gpu 3
[13:06:46.500745] Namespace(batch_size=128, epochs=100, accum_iter=2, model='vit_base_patch16', input_size=224, drop_path=0.1, clip_grad=None, weight_decay=0.05, lr=0.004, blr=0.001, layer_decay=0.75, min_lr=1e-06, warmup_epochs=20, color_jitter=None, aa='rand-m9-mstd0.5-inc1', smoothing=0.1, reprob=0.25, remode='pixel', recount=1, resplit=False, mixup=0.8, cutmix=1.0, cutmix_minmax=None, mixup_prob=1.0, mixup_switch_prob=0.5, mixup_mode='batch', finetune='/raid/home_yedek/utku/freezeout_localmim_rho/ViT/full_pretrain_out/checkpoint-99.pth', global_pool=True, data_path='/raid/utku/datasets/imagenet/classification/', nb_classes=1000, output_dir='full_finetune_out/', log_dir='full_finetune_out', device='cuda', seed=0, resume='', auto_resume=True, start_epoch=0, eval=False, dist_eval=True, num_workers=10, pin_mem=True, world_size=4, local_rank=0, dist_on_itp=False, dist_url='env://', rank=0, gpu=0, distributed=True, dist_backend='nccl')
[13:06:46.501946] Train data path is:  /raid/utku/datasets/imagenet/classification/train/image_folders
[13:06:53.180986] Dataset ImageFolder
    Number of datapoints: 1281167
    Root location: /raid/utku/datasets/imagenet/classification/train/image_folders
    StandardTransform
Transform: Compose(
               RandomResizedCropAndInterpolation(size=(224, 224), scale=(0.08, 1.0), ratio=(0.75, 1.3333), interpolation=bicubic)
               RandomHorizontalFlip(p=0.5)
               RandAugment(n=2, ops=
           	AugmentOp(name=AutoContrast, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=Equalize, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=Invert, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=Rotate, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=PosterizeIncreasing, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=SolarizeIncreasing, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=SolarizeAdd, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=ColorIncreasing, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=ContrastIncreasing, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=BrightnessIncreasing, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=SharpnessIncreasing, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=ShearX, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=ShearY, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=TranslateXRel, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=TranslateYRel, p=0.5, m=9, mstd=0.5))
               ToTensor()
               Normalize(mean=tensor([0.4850, 0.4560, 0.4060]), std=tensor([0.2290, 0.2240, 0.2250]))
               RandomErasing(p=0.25, mode=pixel, count=(1, 1))
           )
[13:06:53.185800] Val data path is:  /raid/utku/datasets/imagenet/classification/val/image_folders
[13:06:53.468406] Dataset ImageFolder
    Number of datapoints: 50000
    Root location: /raid/utku/datasets/imagenet/classification/val/image_folders
    StandardTransform
Transform: Compose(
               Resize(size=256, interpolation=bicubic, max_size=None, antialias=None)
               CenterCrop(size=(224, 224))
               ToTensor()
               Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))
           )
[13:06:53.468662] Sampler_train = <torch.utils.data.distributed.DistributedSampler object at 0x7f308c0d68f0>
[13:06:53.471504] Mixup is activated!
[13:06:55.812347] Load pre-trained checkpoint from: /raid/home_yedek/utku/freezeout_localmim_rho/ViT/full_pretrain_out/checkpoint-99.pth
[13:06:55.920149] _IncompatibleKeys(missing_keys=['fc_norm.weight', 'fc_norm.bias', 'head.weight', 'head.bias'], unexpected_keys=['norm.0.weight', 'norm.0.bias', 'norm.1.weight', 'norm.1.bias', 'norm.2.weight', 'norm.2.bias', 'norm.3.weight', 'norm.3.bias', 'decoder.0.mask_token', 'decoder.0.pos_embed', 'decoder.0.embed.weight', 'decoder.0.embed.bias', 'decoder.0.blocks.0.norm1.weight', 'decoder.0.blocks.0.norm1.bias', 'decoder.0.blocks.0.attn.qkv.weight', 'decoder.0.blocks.0.attn.qkv.bias', 'decoder.0.blocks.0.attn.proj.weight', 'decoder.0.blocks.0.attn.proj.bias', 'decoder.0.blocks.0.norm2.weight', 'decoder.0.blocks.0.norm2.bias', 'decoder.0.blocks.0.mlp.fc1.weight', 'decoder.0.blocks.0.mlp.fc1.bias', 'decoder.0.blocks.0.mlp.fc2.weight', 'decoder.0.blocks.0.mlp.fc2.bias', 'decoder.0.norm.weight', 'decoder.0.norm.bias', 'decoder.0.pred.0.weight', 'decoder.0.pred.0.bias', 'decoder.0.pred.1.weight', 'decoder.0.pred.1.bias', 'decoder.0.pred.3.weight', 'decoder.0.pred.3.bias', 'decoder.0.pred.4.weight', 'decoder.0.pred.4.bias', 'decoder.1.mask_token', 'decoder.1.pos_embed', 'decoder.1.embed.weight', 'decoder.1.embed.bias', 'decoder.1.blocks.0.norm1.weight', 'decoder.1.blocks.0.norm1.bias', 'decoder.1.blocks.0.attn.qkv.weight', 'decoder.1.blocks.0.attn.qkv.bias', 'decoder.1.blocks.0.attn.proj.weight', 'decoder.1.blocks.0.attn.proj.bias', 'decoder.1.blocks.0.norm2.weight', 'decoder.1.blocks.0.norm2.bias', 'decoder.1.blocks.0.mlp.fc1.weight', 'decoder.1.blocks.0.mlp.fc1.bias', 'decoder.1.blocks.0.mlp.fc2.weight', 'decoder.1.blocks.0.mlp.fc2.bias', 'decoder.1.norm.weight', 'decoder.1.norm.bias', 'decoder.1.pred.0.weight', 'decoder.1.pred.0.bias', 'decoder.1.pred.1.weight', 'decoder.1.pred.1.bias', 'decoder.2.mask_token', 'decoder.2.pos_embed', 'decoder.2.embed.weight', 'decoder.2.embed.bias', 'decoder.2.blocks.0.norm1.weight', 'decoder.2.blocks.0.norm1.bias', 'decoder.2.blocks.0.attn.qkv.weight', 'decoder.2.blocks.0.attn.qkv.bias', 'decoder.2.blocks.0.attn.proj.weight', 'decoder.2.blocks.0.attn.proj.bias', 'decoder.2.blocks.0.norm2.weight', 'decoder.2.blocks.0.norm2.bias', 'decoder.2.blocks.0.mlp.fc1.weight', 'decoder.2.blocks.0.mlp.fc1.bias', 'decoder.2.blocks.0.mlp.fc2.weight', 'decoder.2.blocks.0.mlp.fc2.bias', 'decoder.2.norm.weight', 'decoder.2.norm.bias', 'decoder.2.pred.0.weight', 'decoder.2.pred.0.bias', 'decoder.3.mask_token', 'decoder.3.pos_embed', 'decoder.3.embed.weight', 'decoder.3.embed.bias', 'decoder.3.blocks.0.norm1.weight', 'decoder.3.blocks.0.norm1.bias', 'decoder.3.blocks.0.attn.qkv.weight', 'decoder.3.blocks.0.attn.qkv.bias', 'decoder.3.blocks.0.attn.proj.weight', 'decoder.3.blocks.0.attn.proj.bias', 'decoder.3.blocks.0.norm2.weight', 'decoder.3.blocks.0.norm2.bias', 'decoder.3.blocks.0.mlp.fc1.weight', 'decoder.3.blocks.0.mlp.fc1.bias', 'decoder.3.blocks.0.mlp.fc2.weight', 'decoder.3.blocks.0.mlp.fc2.bias', 'decoder.3.norm.weight', 'decoder.3.norm.bias', 'decoder.3.pred.1.weight', 'decoder.3.pred.1.bias', 'hog_enc.0.conv.weight', 'hog_enc.1.conv.weight', 'hog_enc.2.conv.weight', 'hog_enc.3.conv.weight'])
[13:06:56.021216] number of params (M): 86.57
[13:06:56.021287] base lr: 1.00e-03
[13:06:56.021307] actual lr: 4.00e-03
[13:06:56.021323] accumulate grad iterations: 2
[13:06:56.021339] effective batch size: 1024
[13:06:56.144058] parameter groups: 
{
  "layer_0_no_decay": {
    "lr_scale": 0.023757264018058777,
    "weight_decay": 0.0,
    "params": [
      "cls_token",
      "pos_embed",
      "patch_embed.proj.bias"
    ]
  },
  "layer_0_decay": {
    "lr_scale": 0.023757264018058777,
    "weight_decay": 0.05,
    "params": [
      "patch_embed.proj.weight"
    ]
  },
  "layer_1_no_decay": {
    "lr_scale": 0.03167635202407837,
    "weight_decay": 0.0,
    "params": [
      "blocks.0.norm1.weight",
      "blocks.0.norm1.bias",
      "blocks.0.attn.qkv.bias",
      "blocks.0.attn.proj.bias",
      "blocks.0.norm2.weight",
      "blocks.0.norm2.bias",
      "blocks.0.mlp.fc1.bias",
      "blocks.0.mlp.fc2.bias"
    ]
  },
  "layer_1_decay": {
    "lr_scale": 0.03167635202407837,
    "weight_decay": 0.05,
    "params": [
      "blocks.0.attn.qkv.weight",
      "blocks.0.attn.proj.weight",
      "blocks.0.mlp.fc1.weight",
      "blocks.0.mlp.fc2.weight"
    ]
  },
  "layer_2_no_decay": {
    "lr_scale": 0.04223513603210449,
    "weight_decay": 0.0,
    "params": [
      "blocks.1.norm1.weight",
      "blocks.1.norm1.bias",
      "blocks.1.attn.qkv.bias",
      "blocks.1.attn.proj.bias",
      "blocks.1.norm2.weight",
      "blocks.1.norm2.bias",
      "blocks.1.mlp.fc1.bias",
      "blocks.1.mlp.fc2.bias"
    ]
  },
  "layer_2_decay": {
    "lr_scale": 0.04223513603210449,
    "weight_decay": 0.05,
    "params": [
      "blocks.1.attn.qkv.weight",
      "blocks.1.attn.proj.weight",
      "blocks.1.mlp.fc1.weight",
      "blocks.1.mlp.fc2.weight"
    ]
  },
  "layer_3_no_decay": {
    "lr_scale": 0.056313514709472656,
    "weight_decay": 0.0,
    "params": [
      "blocks.2.norm1.weight",
      "blocks.2.norm1.bias",
      "blocks.2.attn.qkv.bias",
      "blocks.2.attn.proj.bias",
      "blocks.2.norm2.weight",
      "blocks.2.norm2.bias",
      "blocks.2.mlp.fc1.bias",
      "blocks.2.mlp.fc2.bias"
    ]
  },
  "layer_3_decay": {
    "lr_scale": 0.056313514709472656,
    "weight_decay": 0.05,
    "params": [
      "blocks.2.attn.qkv.weight",
      "blocks.2.attn.proj.weight",
      "blocks.2.mlp.fc1.weight",
      "blocks.2.mlp.fc2.weight"
    ]
  },
  "layer_4_no_decay": {
    "lr_scale": 0.07508468627929688,
    "weight_decay": 0.0,
    "params": [
      "blocks.3.norm1.weight",
      "blocks.3.norm1.bias",
      "blocks.3.attn.qkv.bias",
      "blocks.3.attn.proj.bias",
      "blocks.3.norm2.weight",
      "blocks.3.norm2.bias",
      "blocks.3.mlp.fc1.bias",
      "blocks.3.mlp.fc2.bias"
    ]
  },
  "layer_4_decay": {
    "lr_scale": 0.07508468627929688,
    "weight_decay": 0.05,
    "params": [
      "blocks.3.attn.qkv.weight",
      "blocks.3.attn.proj.weight",
      "blocks.3.mlp.fc1.weight",
      "blocks.3.mlp.fc2.weight"
    ]
  },
  "layer_5_no_decay": {
    "lr_scale": 0.1001129150390625,
    "weight_decay": 0.0,
    "params": [
      "blocks.4.norm1.weight",
      "blocks.4.norm1.bias",
      "blocks.4.attn.qkv.bias",
      "blocks.4.attn.proj.bias",
      "blocks.4.norm2.weight",
      "blocks.4.norm2.bias",
      "blocks.4.mlp.fc1.bias",
      "blocks.4.mlp.fc2.bias"
    ]
  },
  "layer_5_decay": {
    "lr_scale": 0.1001129150390625,
    "weight_decay": 0.05,
    "params": [
      "blocks.4.attn.qkv.weight",
      "blocks.4.attn.proj.weight",
      "blocks.4.mlp.fc1.weight",
      "blocks.4.mlp.fc2.weight"
    ]
  },
  "layer_6_no_decay": {
    "lr_scale": 0.13348388671875,
    "weight_decay": 0.0,
    "params": [
      "blocks.5.norm1.weight",
      "blocks.5.norm1.bias",
      "blocks.5.attn.qkv.bias",
      "blocks.5.attn.proj.bias",
      "blocks.5.norm2.weight",
      "blocks.5.norm2.bias",
      "blocks.5.mlp.fc1.bias",
      "blocks.5.mlp.fc2.bias"
    ]
  },
  "layer_6_decay": {
    "lr_scale": 0.13348388671875,
    "weight_decay": 0.05,
    "params": [
      "blocks.5.attn.qkv.weight",
      "blocks.5.attn.proj.weight",
      "blocks.5.mlp.fc1.weight",
      "blocks.5.mlp.fc2.weight"
    ]
  },
  "layer_7_no_decay": {
    "lr_scale": 0.177978515625,
    "weight_decay": 0.0,
    "params": [
      "blocks.6.norm1.weight",
      "blocks.6.norm1.bias",
      "blocks.6.attn.qkv.bias",
      "blocks.6.attn.proj.bias",
      "blocks.6.norm2.weight",
      "blocks.6.norm2.bias",
      "blocks.6.mlp.fc1.bias",
      "blocks.6.mlp.fc2.bias"
    ]
  },
  "layer_7_decay": {
    "lr_scale": 0.177978515625,
    "weight_decay": 0.05,
    "params": [
      "blocks.6.attn.qkv.weight",
      "blocks.6.attn.proj.weight",
      "blocks.6.mlp.fc1.weight",
      "blocks.6.mlp.fc2.weight"
    ]
  },
  "layer_8_no_decay": {
    "lr_scale": 0.2373046875,
    "weight_decay": 0.0,
    "params": [
      "blocks.7.norm1.weight",
      "blocks.7.norm1.bias",
      "blocks.7.attn.qkv.bias",
      "blocks.7.attn.proj.bias",
      "blocks.7.norm2.weight",
      "blocks.7.norm2.bias",
      "blocks.7.mlp.fc1.bias",
      "blocks.7.mlp.fc2.bias"
    ]
  },
  "layer_8_decay": {
    "lr_scale": 0.2373046875,
    "weight_decay": 0.05,
    "params": [
      "blocks.7.attn.qkv.weight",
      "blocks.7.attn.proj.weight",
      "blocks.7.mlp.fc1.weight",
      "blocks.7.mlp.fc2.weight"
    ]
  },
  "layer_9_no_decay": {
    "lr_scale": 0.31640625,
    "weight_decay": 0.0,
    "params": [
      "blocks.8.norm1.weight",
      "blocks.8.norm1.bias",
      "blocks.8.attn.qkv.bias",
      "blocks.8.attn.proj.bias",
      "blocks.8.norm2.weight",
      "blocks.8.norm2.bias",
      "blocks.8.mlp.fc1.bias",
      "blocks.8.mlp.fc2.bias"
    ]
  },
  "layer_9_decay": {
    "lr_scale": 0.31640625,
    "weight_decay": 0.05,
    "params": [
      "blocks.8.attn.qkv.weight",
      "blocks.8.attn.proj.weight",
      "blocks.8.mlp.fc1.weight",
      "blocks.8.mlp.fc2.weight"
    ]
  },
  "layer_10_no_decay": {
    "lr_scale": 0.421875,
    "weight_decay": 0.0,
    "params": [
      "blocks.9.norm1.weight",
      "blocks.9.norm1.bias",
      "blocks.9.attn.qkv.bias",
      "blocks.9.attn.proj.bias",
      "blocks.9.norm2.weight",
      "blocks.9.norm2.bias",
      "blocks.9.mlp.fc1.bias",
      "blocks.9.mlp.fc2.bias"
    ]
  },
  "layer_10_decay": {
    "lr_scale": 0.421875,
    "weight_decay": 0.05,
    "params": [
      "blocks.9.attn.qkv.weight",
      "blocks.9.attn.proj.weight",
      "blocks.9.mlp.fc1.weight",
      "blocks.9.mlp.fc2.weight"
    ]
  },
  "layer_11_no_decay": {
    "lr_scale": 0.5625,
    "weight_decay": 0.0,
    "params": [
      "blocks.10.norm1.weight",
      "blocks.10.norm1.bias",
      "blocks.10.attn.qkv.bias",
      "blocks.10.attn.proj.bias",
      "blocks.10.norm2.weight",
      "blocks.10.norm2.bias",
      "blocks.10.mlp.fc1.bias",
      "blocks.10.mlp.fc2.bias"
    ]
  },
  "layer_11_decay": {
    "lr_scale": 0.5625,
    "weight_decay": 0.05,
    "params": [
      "blocks.10.attn.qkv.weight",
      "blocks.10.attn.proj.weight",
      "blocks.10.mlp.fc1.weight",
      "blocks.10.mlp.fc2.weight"
    ]
  },
  "layer_12_no_decay": {
    "lr_scale": 0.75,
    "weight_decay": 0.0,
    "params": [
      "blocks.11.norm1.weight",
      "blocks.11.norm1.bias",
      "blocks.11.attn.qkv.bias",
      "blocks.11.attn.proj.bias",
      "blocks.11.norm2.weight",
      "blocks.11.norm2.bias",
      "blocks.11.mlp.fc1.bias",
      "blocks.11.mlp.fc2.bias"
    ]
  },
  "layer_12_decay": {
    "lr_scale": 0.75,
    "weight_decay": 0.05,
    "params": [
      "blocks.11.attn.qkv.weight",
      "blocks.11.attn.proj.weight",
      "blocks.11.mlp.fc1.weight",
      "blocks.11.mlp.fc2.weight"
    ]
  },
  "layer_13_no_decay": {
    "lr_scale": 1.0,
    "weight_decay": 0.0,
    "params": [
      "fc_norm.weight",
      "fc_norm.bias",
      "head.bias"
    ]
  },
  "layer_13_decay": {
    "lr_scale": 1.0,
    "weight_decay": 0.05,
    "params": [
      "head.weight"
    ]
  }
}
[13:06:56.145219] criterion = SoftTargetCrossEntropy()
[13:06:56.145832] Auto resume checkpoint: 
[13:06:56.145873] Start training for 100 epochs
[13:06:56.147742] log_dir: full_finetune_out
[13:07:06.126729] Epoch: [0]  [   0/2502]  eta: 6:56:03  lr: 0.000000  loss: 6.9078 (6.9078)  time: 9.9773  data: 3.4478  max mem: 13378
[13:07:25.871150] Epoch: [0]  [  50/2502]  eta: 0:23:48  lr: 0.000004  loss: 6.9077 (6.9077)  time: 0.3903  data: 0.0007  max mem: 14369
[13:07:45.465572] Epoch: [0]  [ 100/2502]  eta: 0:19:32  lr: 0.000008  loss: 6.9073 (6.9076)  time: 0.3924  data: 0.0009  max mem: 14369
[13:08:05.158678] Epoch: [0]  [ 150/2502]  eta: 0:17:54  lr: 0.000012  loss: 6.9067 (6.9074)  time: 0.3948  data: 0.0008  max mem: 14369
[13:08:24.862946] Epoch: [0]  [ 200/2502]  eta: 0:16:55  lr: 0.000016  loss: 6.9041 (6.9068)  time: 0.3934  data: 0.0008  max mem: 14369
[13:08:44.504150] Epoch: [0]  [ 250/2502]  eta: 0:16:12  lr: 0.000020  loss: 6.8969 (6.9055)  time: 0.3938  data: 0.0007  max mem: 14369
[13:09:04.378823] Epoch: [0]  [ 300/2502]  eta: 0:15:37  lr: 0.000024  loss: 6.8629 (6.9009)  time: 0.3974  data: 0.0010  max mem: 14369
[13:09:24.294915] Epoch: [0]  [ 350/2502]  eta: 0:15:08  lr: 0.000028  loss: 6.7976 (6.8891)  time: 0.3976  data: 0.0008  max mem: 14369
[13:09:44.049393] Epoch: [0]  [ 400/2502]  eta: 0:14:40  lr: 0.000032  loss: 6.7413 (6.8722)  time: 0.3957  data: 0.0007  max mem: 14369
[13:10:04.172433] Epoch: [0]  [ 450/2502]  eta: 0:14:15  lr: 0.000036  loss: 6.6915 (6.8542)  time: 0.3996  data: 0.0009  max mem: 14369
[13:10:24.080578] Epoch: [0]  [ 500/2502]  eta: 0:13:50  lr: 0.000040  loss: 6.6307 (6.8328)  time: 0.3967  data: 0.0007  max mem: 14369
[13:10:43.931210] Epoch: [0]  [ 550/2502]  eta: 0:13:26  lr: 0.000044  loss: 6.5790 (6.8111)  time: 0.3965  data: 0.0006  max mem: 14369
[13:11:03.847660] Epoch: [0]  [ 600/2502]  eta: 0:13:03  lr: 0.000048  loss: 6.5440 (6.7919)  time: 0.3992  data: 0.0006  max mem: 14369
[13:11:23.694121] Epoch: [0]  [ 650/2502]  eta: 0:12:41  lr: 0.000052  loss: 6.4503 (6.7697)  time: 0.3974  data: 0.0006  max mem: 14369
[13:11:43.567566] Epoch: [0]  [ 700/2502]  eta: 0:12:18  lr: 0.000056  loss: 6.3374 (6.7455)  time: 0.3973  data: 0.0006  max mem: 14369
[13:12:03.431891] Epoch: [0]  [ 750/2502]  eta: 0:11:56  lr: 0.000060  loss: 6.4829 (6.7235)  time: 0.3979  data: 0.0006  max mem: 14369
[13:12:23.342528] Epoch: [0]  [ 800/2502]  eta: 0:11:35  lr: 0.000064  loss: 6.3736 (6.7021)  time: 0.3982  data: 0.0006  max mem: 14369
[13:12:43.240188] Epoch: [0]  [ 850/2502]  eta: 0:11:13  lr: 0.000068  loss: 6.3422 (6.6770)  time: 0.3966  data: 0.0006  max mem: 14369
[13:13:03.110270] Epoch: [0]  [ 900/2502]  eta: 0:10:52  lr: 0.000072  loss: 6.2659 (6.6536)  time: 0.3978  data: 0.0005  max mem: 14369
[13:13:23.036534] Epoch: [0]  [ 950/2502]  eta: 0:10:31  lr: 0.000076  loss: 6.0253 (6.6315)  time: 0.3987  data: 0.0006  max mem: 14369
[13:13:42.938998] Epoch: [0]  [1000/2502]  eta: 0:10:10  lr: 0.000080  loss: 6.2732 (6.6113)  time: 0.3972  data: 0.0006  max mem: 14369
[13:14:02.892668] Epoch: [0]  [1050/2502]  eta: 0:09:49  lr: 0.000084  loss: 6.2980 (6.5899)  time: 0.3983  data: 0.0006  max mem: 14369
[13:14:22.869566] Epoch: [0]  [1100/2502]  eta: 0:09:28  lr: 0.000088  loss: 6.1153 (6.5680)  time: 0.3988  data: 0.0005  max mem: 14369
[13:14:42.794677] Epoch: [0]  [1150/2502]  eta: 0:09:08  lr: 0.000092  loss: 6.0486 (6.5485)  time: 0.3984  data: 0.0005  max mem: 14369
[13:15:02.681396] Epoch: [0]  [1200/2502]  eta: 0:08:47  lr: 0.000096  loss: 6.0781 (6.5309)  time: 0.3980  data: 0.0006  max mem: 14369
[13:15:22.607992] Epoch: [0]  [1250/2502]  eta: 0:08:26  lr: 0.000100  loss: 5.9686 (6.5090)  time: 0.3985  data: 0.0005  max mem: 14369
[13:15:42.573826] Epoch: [0]  [1300/2502]  eta: 0:08:06  lr: 0.000104  loss: 6.1097 (6.4920)  time: 0.3980  data: 0.0005  max mem: 14369
[13:16:02.494024] Epoch: [0]  [1350/2502]  eta: 0:07:45  lr: 0.000108  loss: 6.0243 (6.4710)  time: 0.3976  data: 0.0005  max mem: 14369
[13:16:22.404786] Epoch: [0]  [1400/2502]  eta: 0:07:25  lr: 0.000112  loss: 5.9643 (6.4542)  time: 0.3979  data: 0.0005  max mem: 14369
[13:16:42.336224] Epoch: [0]  [1450/2502]  eta: 0:07:04  lr: 0.000116  loss: 5.9383 (6.4364)  time: 0.3982  data: 0.0005  max mem: 14369
[13:17:02.242257] Epoch: [0]  [1500/2502]  eta: 0:06:44  lr: 0.000120  loss: 5.7303 (6.4150)  time: 0.3997  data: 0.0005  max mem: 14369
[13:17:22.171504] Epoch: [0]  [1550/2502]  eta: 0:06:24  lr: 0.000124  loss: 5.7862 (6.3971)  time: 0.3995  data: 0.0005  max mem: 14369
[13:17:42.021921] Epoch: [0]  [1600/2502]  eta: 0:06:03  lr: 0.000128  loss: 5.8841 (6.3790)  time: 0.3971  data: 0.0005  max mem: 14369
[13:18:01.980111] Epoch: [0]  [1650/2502]  eta: 0:05:43  lr: 0.000132  loss: 5.7249 (6.3592)  time: 0.4003  data: 0.0005  max mem: 14369
[13:18:21.923630] Epoch: [0]  [1700/2502]  eta: 0:05:23  lr: 0.000136  loss: 5.9604 (6.3406)  time: 0.3990  data: 0.0005  max mem: 14369
[13:18:41.871321] Epoch: [0]  [1750/2502]  eta: 0:05:03  lr: 0.000140  loss: 5.5798 (6.3224)  time: 0.3986  data: 0.0006  max mem: 14369
[13:19:01.812223] Epoch: [0]  [1800/2502]  eta: 0:04:42  lr: 0.000144  loss: 5.7749 (6.3063)  time: 0.3990  data: 0.0005  max mem: 14369
[13:19:21.704066] Epoch: [0]  [1850/2502]  eta: 0:04:22  lr: 0.000148  loss: 5.5763 (6.2887)  time: 0.3979  data: 0.0006  max mem: 14369
[13:19:41.618330] Epoch: [0]  [1900/2502]  eta: 0:04:02  lr: 0.000152  loss: 5.5539 (6.2711)  time: 0.3975  data: 0.0005  max mem: 14369
[13:20:01.552926] Epoch: [0]  [1950/2502]  eta: 0:03:42  lr: 0.000156  loss: 5.5270 (6.2518)  time: 0.3971  data: 0.0005  max mem: 14369
[13:20:21.455633] Epoch: [0]  [2000/2502]  eta: 0:03:22  lr: 0.000160  loss: 5.6466 (6.2332)  time: 0.3971  data: 0.0005  max mem: 14369
[13:20:41.379084] Epoch: [0]  [2050/2502]  eta: 0:03:01  lr: 0.000164  loss: 5.5181 (6.2187)  time: 0.3995  data: 0.0005  max mem: 14369
[13:21:01.300338] Epoch: [0]  [2100/2502]  eta: 0:02:41  lr: 0.000168  loss: 5.6896 (6.2025)  time: 0.3974  data: 0.0006  max mem: 14369
[13:21:21.271825] Epoch: [0]  [2150/2502]  eta: 0:02:21  lr: 0.000172  loss: 5.7517 (6.1881)  time: 0.4001  data: 0.0006  max mem: 14369
[13:21:41.178900] Epoch: [0]  [2200/2502]  eta: 0:02:01  lr: 0.000176  loss: 5.5614 (6.1729)  time: 0.3983  data: 0.0006  max mem: 14369
[13:22:01.133161] Epoch: [0]  [2250/2502]  eta: 0:01:41  lr: 0.000180  loss: 5.3362 (6.1550)  time: 0.3991  data: 0.0006  max mem: 14369
[13:22:21.095622] Epoch: [0]  [2300/2502]  eta: 0:01:21  lr: 0.000184  loss: 5.4279 (6.1398)  time: 0.3977  data: 0.0006  max mem: 14369
[13:22:40.999835] Epoch: [0]  [2350/2502]  eta: 0:01:01  lr: 0.000188  loss: 5.4453 (6.1262)  time: 0.3977  data: 0.0005  max mem: 14369
[13:23:00.929814] Epoch: [0]  [2400/2502]  eta: 0:00:40  lr: 0.000192  loss: 5.1821 (6.1105)  time: 0.3985  data: 0.0006  max mem: 14369
[13:23:20.876889] Epoch: [0]  [2450/2502]  eta: 0:00:20  lr: 0.000196  loss: 5.5828 (6.0967)  time: 0.3988  data: 0.0005  max mem: 14369
[13:23:40.671888] Epoch: [0]  [2500/2502]  eta: 0:00:00  lr: 0.000200  loss: 5.4525 (6.0824)  time: 0.3934  data: 0.0015  max mem: 14369
[13:23:41.074260] Epoch: [0]  [2501/2502]  eta: 0:00:00  lr: 0.000200  loss: 5.4525 (6.0818)  time: 0.3927  data: 0.0015  max mem: 14369
[13:23:41.716341] Epoch: [0] Total time: 0:16:45 (0.4019 s / it)
[13:23:41.738237] Averaged stats: lr: 0.000200  loss: 5.4525 (6.0873)
[13:23:46.050923] Test:  [ 0/98]  eta: 0:06:06  loss: 3.2932 (3.2932)  acc1: 33.5938 (33.5938)  acc5: 60.9375 (60.9375)  time: 3.7413  data: 3.6048  max mem: 14369
[13:23:53.744450] Test:  [30/98]  eta: 0:00:25  loss: 3.2822 (3.3054)  acc1: 32.0312 (32.7873)  acc5: 61.7188 (62.8780)  time: 0.2381  data: 0.1069  max mem: 14369
[13:24:01.867137] Test:  [60/98]  eta: 0:00:12  loss: 3.3889 (3.3346)  acc1: 32.0312 (32.2234)  acc5: 61.7188 (62.1030)  time: 0.2681  data: 0.1344  max mem: 14369
[13:24:09.959421] Test:  [90/98]  eta: 0:00:02  loss: 3.3456 (3.3331)  acc1: 32.0312 (32.2201)  acc5: 60.9375 (62.0707)  time: 0.2759  data: 0.1462  max mem: 14369
[13:24:11.190366] Test:  [97/98]  eta: 0:00:00  loss: 3.3224 (3.3298)  acc1: 32.8125 (32.4400)  acc5: 63.2812 (62.1600)  time: 0.2484  data: 0.1053  max mem: 14369
[13:24:11.321253] Test: Total time: 0:00:29 (0.2961 s / it)
[13:24:11.554070] * Acc@1 32.496 Acc@5 61.846 loss 3.324
[13:24:11.554255] Accuracy of the network on the 50000 test images: 32.5%
[13:24:11.554296] Max accuracy: 32.50%
[13:24:11.558916] log_dir: full_finetune_out
[13:24:16.269050] Epoch: [1]  [   0/2502]  eta: 3:16:20  lr: 0.000200  loss: 4.8193 (4.8193)  time: 4.7085  data: 3.9180  max mem: 14369
[13:24:36.416086] Epoch: [1]  [  50/2502]  eta: 0:19:54  lr: 0.000204  loss: 5.4167 (5.4220)  time: 0.3974  data: 0.0007  max mem: 14369
[13:24:56.324941] Epoch: [1]  [ 100/2502]  eta: 0:17:44  lr: 0.000208  loss: 5.4657 (5.4464)  time: 0.3978  data: 0.0007  max mem: 14369
[13:25:16.220720] Epoch: [1]  [ 150/2502]  eta: 0:16:47  lr: 0.000212  loss: 5.6832 (5.4080)  time: 0.3986  data: 0.0007  max mem: 14369
[13:25:36.143535] Epoch: [1]  [ 200/2502]  eta: 0:16:08  lr: 0.000216  loss: 5.5968 (5.4190)  time: 0.4006  data: 0.0009  max mem: 14369
[13:25:56.093193] Epoch: [1]  [ 250/2502]  eta: 0:15:37  lr: 0.000220  loss: 4.9527 (5.3985)  time: 0.3988  data: 0.0007  max mem: 14369
[13:26:16.031867] Epoch: [1]  [ 300/2502]  eta: 0:15:10  lr: 0.000224  loss: 5.1592 (5.3776)  time: 0.3991  data: 0.0007  max mem: 14369
[13:26:35.876141] Epoch: [1]  [ 350/2502]  eta: 0:14:44  lr: 0.000228  loss: 5.1589 (5.3598)  time: 0.3924  data: 0.0005  max mem: 14369
[13:26:55.775016] Epoch: [1]  [ 400/2502]  eta: 0:14:20  lr: 0.000232  loss: 5.2884 (5.3256)  time: 0.3986  data: 0.0007  max mem: 14369
[13:27:15.696544] Epoch: [1]  [ 450/2502]  eta: 0:13:57  lr: 0.000236  loss: 5.2803 (5.3206)  time: 0.3995  data: 0.0007  max mem: 14369
[13:27:35.576493] Epoch: [1]  [ 500/2502]  eta: 0:13:35  lr: 0.000240  loss: 5.4528 (5.3140)  time: 0.3979  data: 0.0006  max mem: 14369
[13:27:55.464621] Epoch: [1]  [ 550/2502]  eta: 0:13:13  lr: 0.000244  loss: 5.5165 (5.3080)  time: 0.3962  data: 0.0008  max mem: 14369
[13:28:15.326464] Epoch: [1]  [ 600/2502]  eta: 0:12:51  lr: 0.000248  loss: 5.3419 (5.2945)  time: 0.3959  data: 0.0006  max mem: 14369
[13:28:35.224082] Epoch: [1]  [ 650/2502]  eta: 0:12:29  lr: 0.000252  loss: 5.0017 (5.2828)  time: 0.3985  data: 0.0006  max mem: 14369
[13:28:55.113358] Epoch: [1]  [ 700/2502]  eta: 0:12:08  lr: 0.000256  loss: 5.2887 (5.2758)  time: 0.3981  data: 0.0006  max mem: 14369
[13:29:15.001723] Epoch: [1]  [ 750/2502]  eta: 0:11:47  lr: 0.000260  loss: 5.6477 (5.2789)  time: 0.3973  data: 0.0005  max mem: 14369
[13:29:34.918646] Epoch: [1]  [ 800/2502]  eta: 0:11:27  lr: 0.000264  loss: 5.4558 (5.2739)  time: 0.3978  data: 0.0006  max mem: 14369
[13:29:54.802475] Epoch: [1]  [ 850/2502]  eta: 0:11:06  lr: 0.000268  loss: 5.1881 (5.2638)  time: 0.3966  data: 0.0005  max mem: 14369
[13:30:14.665492] Epoch: [1]  [ 900/2502]  eta: 0:10:45  lr: 0.000272  loss: 5.3118 (5.2566)  time: 0.3966  data: 0.0006  max mem: 14369
[13:30:34.591225] Epoch: [1]  [ 950/2502]  eta: 0:10:25  lr: 0.000276  loss: 5.1275 (5.2493)  time: 0.3976  data: 0.0006  max mem: 14369
[13:30:54.510233] Epoch: [1]  [1000/2502]  eta: 0:10:04  lr: 0.000280  loss: 5.0218 (5.2392)  time: 0.3980  data: 0.0005  max mem: 14369
[13:31:14.448447] Epoch: [1]  [1050/2502]  eta: 0:09:44  lr: 0.000284  loss: 4.9683 (5.2293)  time: 0.3983  data: 0.0005  max mem: 14369
[13:31:34.378541] Epoch: [1]  [1100/2502]  eta: 0:09:23  lr: 0.000288  loss: 5.2669 (5.2267)  time: 0.3981  data: 0.0006  max mem: 14369
[13:31:54.256199] Epoch: [1]  [1150/2502]  eta: 0:09:03  lr: 0.000292  loss: 5.1553 (5.2278)  time: 0.3975  data: 0.0006  max mem: 14369
[13:32:14.182642] Epoch: [1]  [1200/2502]  eta: 0:08:43  lr: 0.000296  loss: 5.2162 (5.2223)  time: 0.3983  data: 0.0006  max mem: 14369
[13:32:34.104126] Epoch: [1]  [1250/2502]  eta: 0:08:22  lr: 0.000300  loss: 5.2769 (5.2183)  time: 0.3989  data: 0.0006  max mem: 14369
[13:32:53.994835] Epoch: [1]  [1300/2502]  eta: 0:08:02  lr: 0.000304  loss: 5.2328 (5.2167)  time: 0.3982  data: 0.0005  max mem: 14369
[13:33:13.883400] Epoch: [1]  [1350/2502]  eta: 0:07:42  lr: 0.000308  loss: 5.3128 (5.2112)  time: 0.3972  data: 0.0005  max mem: 14369
[13:33:33.777495] Epoch: [1]  [1400/2502]  eta: 0:07:22  lr: 0.000312  loss: 5.0691 (5.2003)  time: 0.3971  data: 0.0006  max mem: 14369
[13:33:53.718412] Epoch: [1]  [1450/2502]  eta: 0:07:02  lr: 0.000316  loss: 4.6409 (5.1919)  time: 0.3985  data: 0.0005  max mem: 14369
[13:34:13.630013] Epoch: [1]  [1500/2502]  eta: 0:06:41  lr: 0.000320  loss: 4.8782 (5.1880)  time: 0.3974  data: 0.0005  max mem: 14369
[13:34:33.518974] Epoch: [1]  [1550/2502]  eta: 0:06:21  lr: 0.000324  loss: 5.3252 (5.1846)  time: 0.3975  data: 0.0005  max mem: 14369
[13:34:53.389873] Epoch: [1]  [1600/2502]  eta: 0:06:01  lr: 0.000328  loss: 4.8618 (5.1728)  time: 0.3966  data: 0.0006  max mem: 14369
[13:35:13.295890] Epoch: [1]  [1650/2502]  eta: 0:05:41  lr: 0.000332  loss: 4.9660 (5.1668)  time: 0.3980  data: 0.0006  max mem: 14369
[13:35:33.253999] Epoch: [1]  [1700/2502]  eta: 0:05:21  lr: 0.000336  loss: 5.0526 (5.1624)  time: 0.3990  data: 0.0006  max mem: 14369
[13:35:53.190929] Epoch: [1]  [1750/2502]  eta: 0:05:01  lr: 0.000340  loss: 5.1915 (5.1578)  time: 0.3995  data: 0.0005  max mem: 14369
[13:36:13.139734] Epoch: [1]  [1800/2502]  eta: 0:04:41  lr: 0.000344  loss: 4.9014 (5.1501)  time: 0.3989  data: 0.0006  max mem: 14369
[13:36:33.049941] Epoch: [1]  [1850/2502]  eta: 0:04:21  lr: 0.000348  loss: 4.9232 (5.1450)  time: 0.3972  data: 0.0005  max mem: 14369
[13:36:53.015819] Epoch: [1]  [1900/2502]  eta: 0:04:01  lr: 0.000352  loss: 5.2747 (5.1423)  time: 0.3992  data: 0.0005  max mem: 14369
[13:37:12.923051] Epoch: [1]  [1950/2502]  eta: 0:03:41  lr: 0.000356  loss: 5.3040 (5.1373)  time: 0.3977  data: 0.0005  max mem: 14369
[13:37:32.874052] Epoch: [1]  [2000/2502]  eta: 0:03:21  lr: 0.000360  loss: 4.8539 (5.1327)  time: 0.3979  data: 0.0006  max mem: 14369
[13:37:52.772112] Epoch: [1]  [2050/2502]  eta: 0:03:00  lr: 0.000364  loss: 5.0874 (5.1295)  time: 0.3993  data: 0.0006  max mem: 14369
[13:38:12.677940] Epoch: [1]  [2100/2502]  eta: 0:02:40  lr: 0.000368  loss: 4.9774 (5.1269)  time: 0.3988  data: 0.0006  max mem: 14369
[13:38:32.632994] Epoch: [1]  [2150/2502]  eta: 0:02:20  lr: 0.000372  loss: 5.0443 (5.1229)  time: 0.4000  data: 0.0006  max mem: 14369
[13:38:52.564682] Epoch: [1]  [2200/2502]  eta: 0:02:00  lr: 0.000376  loss: 5.2972 (5.1231)  time: 0.3989  data: 0.0005  max mem: 14369
[13:39:12.503064] Epoch: [1]  [2250/2502]  eta: 0:01:40  lr: 0.000380  loss: 5.1417 (5.1189)  time: 0.3987  data: 0.0005  max mem: 14369
[13:39:32.457274] Epoch: [1]  [2300/2502]  eta: 0:01:20  lr: 0.000384  loss: 5.1761 (5.1141)  time: 0.3987  data: 0.0005  max mem: 14369
[13:39:52.387112] Epoch: [1]  [2350/2502]  eta: 0:01:00  lr: 0.000388  loss: 5.1357 (5.1092)  time: 0.3967  data: 0.0006  max mem: 14369
[13:40:12.308829] Epoch: [1]  [2400/2502]  eta: 0:00:40  lr: 0.000392  loss: 4.5951 (5.1006)  time: 0.3975  data: 0.0006  max mem: 14369
[13:40:32.208863] Epoch: [1]  [2450/2502]  eta: 0:00:20  lr: 0.000396  loss: 5.2589 (5.0978)  time: 0.3981  data: 0.0006  max mem: 14369
[13:40:51.946593] Epoch: [1]  [2500/2502]  eta: 0:00:00  lr: 0.000400  loss: 4.9707 (5.0956)  time: 0.3913  data: 0.0032  max mem: 14369
[13:40:52.345423] Epoch: [1]  [2501/2502]  eta: 0:00:00  lr: 0.000400  loss: 4.9707 (5.0959)  time: 0.3903  data: 0.0032  max mem: 14369
[13:40:52.987400] Epoch: [1] Total time: 0:16:41 (0.4003 s / it)
[13:40:52.988347] Averaged stats: lr: 0.000400  loss: 4.9707 (5.0957)
[13:40:56.878244] Test:  [ 0/98]  eta: 0:06:20  loss: 2.2267 (2.2267)  acc1: 52.3438 (52.3438)  acc5: 75.7812 (75.7812)  time: 3.8853  data: 3.7408  max mem: 14369
[13:41:03.627264] Test:  [30/98]  eta: 0:00:23  loss: 2.2000 (2.2027)  acc1: 50.7812 (52.0413)  acc5: 78.9062 (78.8306)  time: 0.2030  data: 0.0729  max mem: 14369
[13:41:11.952031] Test:  [60/98]  eta: 0:00:11  loss: 2.3165 (2.2400)  acc1: 49.2188 (50.9477)  acc5: 78.1250 (78.4708)  time: 0.3132  data: 0.1835  max mem: 14369
[13:41:19.752621] Test:  [90/98]  eta: 0:00:02  loss: 2.2444 (2.2346)  acc1: 50.0000 (50.9100)  acc5: 76.5625 (78.2194)  time: 0.2687  data: 0.1393  max mem: 14369
[13:41:20.602951] Test:  [97/98]  eta: 0:00:00  loss: 2.2131 (2.2299)  acc1: 50.0000 (51.0400)  acc5: 77.3438 (78.2640)  time: 0.2429  data: 0.1165  max mem: 14369
[13:41:20.769886] Test: Total time: 0:00:27 (0.2835 s / it)
[13:41:20.998113] * Acc@1 51.144 Acc@5 78.150 loss 2.217
[13:41:20.998301] Accuracy of the network on the 50000 test images: 51.1%
[13:41:20.998330] Max accuracy: 51.14%
[13:41:21.002955] log_dir: full_finetune_out
[13:41:25.087602] Epoch: [2]  [   0/2502]  eta: 2:50:16  lr: 0.000400  loss: 5.3359 (5.3359)  time: 4.0832  data: 3.0390  max mem: 14369
[13:41:45.379899] Epoch: [2]  [  50/2502]  eta: 0:19:31  lr: 0.000404  loss: 4.9536 (4.8021)  time: 0.3983  data: 0.0006  max mem: 14369
[13:42:05.214993] Epoch: [2]  [ 100/2502]  eta: 0:17:31  lr: 0.000408  loss: 4.8216 (4.8086)  time: 0.3965  data: 0.0005  max mem: 14369
[13:42:25.096078] Epoch: [2]  [ 150/2502]  eta: 0:16:38  lr: 0.000412  loss: 5.0618 (4.8266)  time: 0.3962  data: 0.0006  max mem: 14369
[13:42:44.902335] Epoch: [2]  [ 200/2502]  eta: 0:16:00  lr: 0.000416  loss: 4.9839 (4.8461)  time: 0.3958  data: 0.0006  max mem: 14369
[13:43:04.812483] Epoch: [2]  [ 250/2502]  eta: 0:15:31  lr: 0.000420  loss: 5.2205 (4.8682)  time: 0.3980  data: 0.0006  max mem: 14369
[13:43:24.599016] Epoch: [2]  [ 300/2502]  eta: 0:15:04  lr: 0.000424  loss: 5.0430 (4.8788)  time: 0.3926  data: 0.0005  max mem: 14369
[13:43:44.582078] Epoch: [2]  [ 350/2502]  eta: 0:14:40  lr: 0.000428  loss: 5.1338 (4.8696)  time: 0.4008  data: 0.0008  max mem: 14369
[13:44:04.438447] Epoch: [2]  [ 400/2502]  eta: 0:14:16  lr: 0.000432  loss: 5.0606 (4.8641)  time: 0.3976  data: 0.0006  max mem: 14369
[13:44:24.213103] Epoch: [2]  [ 450/2502]  eta: 0:13:53  lr: 0.000436  loss: 5.0925 (4.8582)  time: 0.3940  data: 0.0006  max mem: 14369
[13:44:43.997199] Epoch: [2]  [ 500/2502]  eta: 0:13:31  lr: 0.000440  loss: 4.9642 (4.8575)  time: 0.3952  data: 0.0006  max mem: 14369
[13:45:03.796904] Epoch: [2]  [ 550/2502]  eta: 0:13:09  lr: 0.000444  loss: 4.8972 (4.8453)  time: 0.3952  data: 0.0006  max mem: 14369
[13:45:23.548597] Epoch: [2]  [ 600/2502]  eta: 0:12:47  lr: 0.000448  loss: 4.9649 (4.8398)  time: 0.3958  data: 0.0006  max mem: 14369
[13:45:43.341360] Epoch: [2]  [ 650/2502]  eta: 0:12:26  lr: 0.000452  loss: 4.8486 (4.8420)  time: 0.3952  data: 0.0006  max mem: 14369
[13:46:03.101642] Epoch: [2]  [ 700/2502]  eta: 0:12:05  lr: 0.000456  loss: 5.0367 (4.8431)  time: 0.3961  data: 0.0006  max mem: 14369
[13:46:22.900502] Epoch: [2]  [ 750/2502]  eta: 0:11:44  lr: 0.000460  loss: 4.8994 (4.8332)  time: 0.3965  data: 0.0006  max mem: 14369
[13:46:42.659810] Epoch: [2]  [ 800/2502]  eta: 0:11:23  lr: 0.000464  loss: 4.7616 (4.8294)  time: 0.3941  data: 0.0006  max mem: 14369
[13:47:02.464544] Epoch: [2]  [ 850/2502]  eta: 0:11:02  lr: 0.000468  loss: 4.7520 (4.8312)  time: 0.3966  data: 0.0006  max mem: 14369
[13:47:22.296631] Epoch: [2]  [ 900/2502]  eta: 0:10:42  lr: 0.000472  loss: 4.6025 (4.8355)  time: 0.3959  data: 0.0006  max mem: 14369
[13:47:42.088689] Epoch: [2]  [ 950/2502]  eta: 0:10:21  lr: 0.000476  loss: 5.0246 (4.8383)  time: 0.3957  data: 0.0006  max mem: 14369
[13:48:01.889453] Epoch: [2]  [1000/2502]  eta: 0:10:01  lr: 0.000480  loss: 4.8413 (4.8383)  time: 0.3971  data: 0.0006  max mem: 14369
[13:48:21.717675] Epoch: [2]  [1050/2502]  eta: 0:09:41  lr: 0.000484  loss: 4.7112 (4.8314)  time: 0.3959  data: 0.0005  max mem: 14369
[13:48:41.504937] Epoch: [2]  [1100/2502]  eta: 0:09:20  lr: 0.000488  loss: 4.9995 (4.8313)  time: 0.3959  data: 0.0006  max mem: 14369
[13:49:01.388499] Epoch: [2]  [1150/2502]  eta: 0:09:00  lr: 0.000492  loss: 4.3858 (4.8225)  time: 0.3974  data: 0.0005  max mem: 14369
[13:49:21.193291] Epoch: [2]  [1200/2502]  eta: 0:08:40  lr: 0.000496  loss: 4.9883 (4.8238)  time: 0.3972  data: 0.0007  max mem: 14369
[13:49:41.048777] Epoch: [2]  [1250/2502]  eta: 0:08:20  lr: 0.000500  loss: 5.0372 (4.8205)  time: 0.3961  data: 0.0006  max mem: 14369
[13:50:00.837965] Epoch: [2]  [1300/2502]  eta: 0:08:00  lr: 0.000504  loss: 5.1070 (4.8257)  time: 0.3975  data: 0.0006  max mem: 14369
[13:50:20.686201] Epoch: [2]  [1350/2502]  eta: 0:07:40  lr: 0.000508  loss: 4.9724 (4.8251)  time: 0.3975  data: 0.0006  max mem: 14369
[13:50:40.567228] Epoch: [2]  [1400/2502]  eta: 0:07:20  lr: 0.000512  loss: 4.6960 (4.8248)  time: 0.3972  data: 0.0006  max mem: 14369
[13:51:00.417718] Epoch: [2]  [1450/2502]  eta: 0:07:00  lr: 0.000516  loss: 4.9306 (4.8201)  time: 0.3972  data: 0.0006  max mem: 14369
[13:51:20.246893] Epoch: [2]  [1500/2502]  eta: 0:06:39  lr: 0.000520  loss: 4.9576 (4.8125)  time: 0.3972  data: 0.0006  max mem: 14369
[13:51:40.107610] Epoch: [2]  [1550/2502]  eta: 0:06:19  lr: 0.000524  loss: 4.9574 (4.8117)  time: 0.3961  data: 0.0006  max mem: 14369
[13:52:00.004528] Epoch: [2]  [1600/2502]  eta: 0:05:59  lr: 0.000528  loss: 4.9160 (4.8093)  time: 0.3973  data: 0.0006  max mem: 14369
[13:52:19.890683] Epoch: [2]  [1650/2502]  eta: 0:05:39  lr: 0.000532  loss: 4.6023 (4.8059)  time: 0.3989  data: 0.0006  max mem: 14369
[13:52:39.742467] Epoch: [2]  [1700/2502]  eta: 0:05:19  lr: 0.000536  loss: 4.9021 (4.8011)  time: 0.3977  data: 0.0005  max mem: 14369
[13:52:59.637275] Epoch: [2]  [1750/2502]  eta: 0:05:00  lr: 0.000540  loss: 4.4111 (4.7974)  time: 0.3969  data: 0.0006  max mem: 14369
[13:53:19.537521] Epoch: [2]  [1800/2502]  eta: 0:04:40  lr: 0.000544  loss: 5.0042 (4.7963)  time: 0.3988  data: 0.0006  max mem: 14369
[13:53:39.459852] Epoch: [2]  [1850/2502]  eta: 0:04:20  lr: 0.000548  loss: 4.4927 (4.7919)  time: 0.3986  data: 0.0006  max mem: 14369
[13:53:59.329931] Epoch: [2]  [1900/2502]  eta: 0:04:00  lr: 0.000552  loss: 4.8201 (4.7879)  time: 0.3975  data: 0.0005  max mem: 14369
[13:54:19.182162] Epoch: [2]  [1950/2502]  eta: 0:03:40  lr: 0.000556  loss: 4.5658 (4.7832)  time: 0.3971  data: 0.0006  max mem: 14369
[13:54:39.103408] Epoch: [2]  [2000/2502]  eta: 0:03:20  lr: 0.000560  loss: 4.3655 (4.7783)  time: 0.3995  data: 0.0006  max mem: 14369
[13:54:58.965549] Epoch: [2]  [2050/2502]  eta: 0:03:00  lr: 0.000564  loss: 4.7177 (4.7742)  time: 0.3979  data: 0.0006  max mem: 14369
[13:55:18.822053] Epoch: [2]  [2100/2502]  eta: 0:02:40  lr: 0.000568  loss: 4.8058 (4.7721)  time: 0.3975  data: 0.0006  max mem: 14369
[13:55:38.617896] Epoch: [2]  [2150/2502]  eta: 0:02:20  lr: 0.000572  loss: 5.0927 (4.7719)  time: 0.3957  data: 0.0005  max mem: 14369
[13:55:58.480612] Epoch: [2]  [2200/2502]  eta: 0:02:00  lr: 0.000576  loss: 4.7192 (4.7703)  time: 0.3970  data: 0.0006  max mem: 14369
[13:56:18.338881] Epoch: [2]  [2250/2502]  eta: 0:01:40  lr: 0.000580  loss: 4.8224 (4.7682)  time: 0.3974  data: 0.0006  max mem: 14369
[13:56:38.174617] Epoch: [2]  [2300/2502]  eta: 0:01:20  lr: 0.000584  loss: 4.7332 (4.7674)  time: 0.3969  data: 0.0006  max mem: 14369
[13:56:58.090804] Epoch: [2]  [2350/2502]  eta: 0:01:00  lr: 0.000588  loss: 4.9085 (4.7642)  time: 0.3986  data: 0.0006  max mem: 14369
[13:57:17.955502] Epoch: [2]  [2400/2502]  eta: 0:00:40  lr: 0.000592  loss: 4.6943 (4.7616)  time: 0.3981  data: 0.0006  max mem: 14369
[13:57:37.856660] Epoch: [2]  [2450/2502]  eta: 0:00:20  lr: 0.000596  loss: 4.4608 (4.7588)  time: 0.3979  data: 0.0006  max mem: 14369
[13:57:57.582457] Epoch: [2]  [2500/2502]  eta: 0:00:00  lr: 0.000600  loss: 4.6187 (4.7568)  time: 0.3914  data: 0.0014  max mem: 14369
[13:57:57.984463] Epoch: [2]  [2501/2502]  eta: 0:00:00  lr: 0.000600  loss: 4.6535 (4.7567)  time: 0.3904  data: 0.0014  max mem: 14369
[13:57:58.665555] Epoch: [2] Total time: 0:16:37 (0.3987 s / it)
[13:57:58.666651] Averaged stats: lr: 0.000600  loss: 4.6535 (4.7542)
[13:58:02.844376] Test:  [ 0/98]  eta: 0:06:48  loss: 1.8915 (1.8915)  acc1: 57.0312 (57.0312)  acc5: 81.2500 (81.2500)  time: 4.1732  data: 4.0386  max mem: 14369
[13:58:09.858461] Test:  [30/98]  eta: 0:00:24  loss: 1.8501 (1.8650)  acc1: 57.8125 (57.9889)  acc5: 84.3750 (83.5938)  time: 0.2186  data: 0.0892  max mem: 14369
[13:58:17.266895] Test:  [60/98]  eta: 0:00:11  loss: 1.9378 (1.8909)  acc1: 57.0312 (57.6076)  acc5: 81.2500 (83.1071)  time: 0.2558  data: 0.1249  max mem: 14369
[13:58:25.616756] Test:  [90/98]  eta: 0:00:02  loss: 1.8694 (1.8812)  acc1: 58.5938 (57.8898)  acc5: 83.5938 (83.2246)  time: 0.2896  data: 0.1583  max mem: 14369
[13:58:26.676670] Test:  [97/98]  eta: 0:00:00  loss: 1.8531 (1.8785)  acc1: 59.5238 (58.0720)  acc5: 83.5938 (83.2320)  time: 0.2371  data: 0.1090  max mem: 14369
[13:58:26.872515] Test: Total time: 0:00:28 (0.2878 s / it)
[13:58:26.904693] * Acc@1 58.140 Acc@5 83.138 loss 1.869
[13:58:26.904946] Accuracy of the network on the 50000 test images: 58.1%
[13:58:26.904982] Max accuracy: 58.14%
[13:58:26.911522] log_dir: full_finetune_out
[13:58:31.163893] Epoch: [3]  [   0/2502]  eta: 2:57:16  lr: 0.000600  loss: 3.5298 (3.5298)  time: 4.2512  data: 3.8830  max mem: 14369
[13:58:51.067800] Epoch: [3]  [  50/2502]  eta: 0:19:21  lr: 0.000604  loss: 4.7595 (4.5269)  time: 0.3961  data: 0.0006  max mem: 14369
[13:59:10.948711] Epoch: [3]  [ 100/2502]  eta: 0:17:27  lr: 0.000608  loss: 4.8489 (4.5623)  time: 0.3972  data: 0.0006  max mem: 14369
[13:59:30.814788] Epoch: [3]  [ 150/2502]  eta: 0:16:35  lr: 0.000612  loss: 4.6698 (4.5838)  time: 0.3978  data: 0.0005  max mem: 14369
[13:59:50.645589] Epoch: [3]  [ 200/2502]  eta: 0:15:58  lr: 0.000616  loss: 4.9311 (4.6131)  time: 0.3969  data: 0.0006  max mem: 14369
[14:00:10.539749] Epoch: [3]  [ 250/2502]  eta: 0:15:29  lr: 0.000620  loss: 4.5383 (4.5848)  time: 0.3971  data: 0.0006  max mem: 14369
[14:00:30.377103] Epoch: [3]  [ 300/2502]  eta: 0:15:03  lr: 0.000624  loss: 4.7324 (4.5869)  time: 0.3992  data: 0.0008  max mem: 14369
[14:00:50.339411] Epoch: [3]  [ 350/2502]  eta: 0:14:39  lr: 0.000628  loss: 4.7789 (4.5868)  time: 0.3996  data: 0.0007  max mem: 14369
[14:01:10.231882] Epoch: [3]  [ 400/2502]  eta: 0:14:16  lr: 0.000632  loss: 4.8620 (4.6085)  time: 0.3962  data: 0.0005  max mem: 14369
[14:01:30.091435] Epoch: [3]  [ 450/2502]  eta: 0:13:53  lr: 0.000636  loss: 4.6604 (4.6039)  time: 0.3964  data: 0.0006  max mem: 14369
[14:01:49.937413] Epoch: [3]  [ 500/2502]  eta: 0:13:31  lr: 0.000640  loss: 4.7710 (4.5950)  time: 0.3972  data: 0.0006  max mem: 14369
[14:02:09.828450] Epoch: [3]  [ 550/2502]  eta: 0:13:09  lr: 0.000644  loss: 4.3619 (4.5810)  time: 0.3972  data: 0.0006  max mem: 14369
[14:02:29.692135] Epoch: [3]  [ 600/2502]  eta: 0:12:48  lr: 0.000648  loss: 4.7677 (4.5888)  time: 0.3971  data: 0.0006  max mem: 14369
[14:02:49.524827] Epoch: [3]  [ 650/2502]  eta: 0:12:27  lr: 0.000652  loss: 4.6409 (4.5875)  time: 0.3972  data: 0.0006  max mem: 14369
[14:03:09.362308] Epoch: [3]  [ 700/2502]  eta: 0:12:05  lr: 0.000656  loss: 4.8267 (4.5888)  time: 0.3963  data: 0.0006  max mem: 14369
[14:03:29.239959] Epoch: [3]  [ 750/2502]  eta: 0:11:45  lr: 0.000660  loss: 4.9838 (4.5843)  time: 0.3982  data: 0.0006  max mem: 14369
[14:03:49.096454] Epoch: [3]  [ 800/2502]  eta: 0:11:24  lr: 0.000664  loss: 4.4819 (4.5862)  time: 0.3968  data: 0.0005  max mem: 14369
[14:04:08.956896] Epoch: [3]  [ 850/2502]  eta: 0:11:03  lr: 0.000668  loss: 4.6377 (4.5861)  time: 0.3983  data: 0.0006  max mem: 14369
[14:04:28.801377] Epoch: [3]  [ 900/2502]  eta: 0:10:43  lr: 0.000672  loss: 4.6637 (4.5896)  time: 0.3973  data: 0.0006  max mem: 14369
[14:04:48.680293] Epoch: [3]  [ 950/2502]  eta: 0:10:22  lr: 0.000676  loss: 4.9721 (4.5917)  time: 0.3973  data: 0.0006  max mem: 14369
[14:05:08.486041] Epoch: [3]  [1000/2502]  eta: 0:10:02  lr: 0.000680  loss: 4.8773 (4.5936)  time: 0.3959  data: 0.0006  max mem: 14369
[14:05:28.308371] Epoch: [3]  [1050/2502]  eta: 0:09:42  lr: 0.000684  loss: 4.8396 (4.5983)  time: 0.3961  data: 0.0006  max mem: 14369
[14:05:48.177055] Epoch: [3]  [1100/2502]  eta: 0:09:21  lr: 0.000688  loss: 4.9077 (4.5983)  time: 0.3989  data: 0.0005  max mem: 14369
[14:06:08.035303] Epoch: [3]  [1150/2502]  eta: 0:09:01  lr: 0.000692  loss: 4.6451 (4.5964)  time: 0.3983  data: 0.0006  max mem: 14369
[14:06:27.899384] Epoch: [3]  [1200/2502]  eta: 0:08:41  lr: 0.000696  loss: 4.6231 (4.5912)  time: 0.3968  data: 0.0006  max mem: 14369
[14:06:47.747161] Epoch: [3]  [1250/2502]  eta: 0:08:21  lr: 0.000700  loss: 4.6988 (4.5948)  time: 0.3958  data: 0.0006  max mem: 14369
[14:07:07.625742] Epoch: [3]  [1300/2502]  eta: 0:08:01  lr: 0.000704  loss: 4.4584 (4.5942)  time: 0.3969  data: 0.0006  max mem: 14369
[14:07:27.458851] Epoch: [3]  [1350/2502]  eta: 0:07:40  lr: 0.000708  loss: 4.7617 (4.5959)  time: 0.3959  data: 0.0006  max mem: 14369
