/opt/conda/lib/python3.10/site-packages/torch/distributed/launch.py:180: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use_env is set by default in torchrun.
If your script expects `--local_rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
| distributed init (rank 0): env://, gpu 0
| distributed init (rank 1): env://, gpu 1
[07:58:46.302577] Namespace(batch_size=32, epochs=9, accum_iter=1, debug=True, model='MIM_vit_base_patch16', input_size=224, mask_ratio=0.75, hog_nbins=9, hog_bias=False, dont_freeze_pe=False, all_stages=False, not_scale_lr=False, non_layerwise_lr=False, how_scale='cubic', t_0=0.8, weight_decay=0.05, lr=None, blr=0.0002, min_lr=1e-06, warmup_epochs=1, data_path='/raid/utku/datasets/imagenet/classification/train/demo_dataset', output_dir='pretrain/debug', log_dir='debug', device='cuda', seed=0, resume='', auto_resume=False, start_epoch=0, num_workers=12, pin_mem=True, world_size=2, local_rank=0, dist_on_itp=False, dist_url='env://', rank=0, gpu=0, distributed=True, dist_backend='nccl')
[07:58:46.362965] 12000
[07:58:46.363069] Sampler_train = <torch.utils.data.distributed.DistributedSampler object at 0x7f46dca4f520>
[07:58:46.365702] base lr: 2.00e-04
[07:58:46.365739] actual lr: 5.00e-05
[07:58:46.365757] accumulate grad iterations: 1
[07:58:46.365772] effective batch size: 64
[07:58:46.365884] Scaling method is cubic and t_0 is 0.8
[07:58:48.387487] Set module with layer_index: 0
[07:58:48.387621] Set module with layer_index: 1
[07:58:48.387809] Set module with layer_index: 2
[07:58:48.387979] Set module with layer_index: 3
[07:58:48.388147] Set module with layer_index: 4
[07:58:48.388314] Set module with layer_index: 5
[07:58:48.388474] Set module with layer_index: 6
[07:58:48.388647] Set module with layer_index: 7
[07:58:48.388808] Set module with layer_index: 8
[07:58:48.388967] Set module with layer_index: 9
[07:58:48.389129] Set module with layer_index: 10
[07:58:48.389314] Set module with layer_index: 11
[07:58:48.389482] Set module with layer_index: 12
[07:58:48.389648] Set module with layer_index: 2
[07:58:48.389696] Set module with layer_index: 4
[07:58:48.389741] Set module with layer_index: 10
[07:58:48.389784] Set module with layer_index: 12
[07:58:48.389836] Set module with layer_index: 2
[07:58:48.390071] Set module with layer_index: 4
[07:58:48.390289] Set module with layer_index: 10
[07:58:48.390488] Set module with layer_index: 12
[07:58:48.390697] Set module with layer_index: 2
[07:58:48.390758] Set module with layer_index: 4
[07:58:48.390815] Set module with layer_index: 10
[07:58:48.390872] Set module with layer_index: 12
[07:58:48.390931] freezeout_module_level_specifier_count:  25
[07:58:48.390956] freezeout_layer_count_vitb_modified:  25
[07:58:48.530037] Number of leaf parameter is: 248
Traceback (most recent call last):
  File "/raid/home_yedek/utku/ViTFreeze/ViT/run_pretrain.py", line 262, in <module>
    main(args)
  File "/raid/home_yedek/utku/ViTFreeze/ViT/run_pretrain.py", line 223, in main
    writer.write(optimizer_param_groups)
TypeError: write() argument must be str, not list
Traceback (most recent call last):
  File "/raid/home_yedek/utku/ViTFreeze/ViT/run_pretrain.py", line 262, in <module>
    main(args)
  File "/raid/home_yedek/utku/ViTFreeze/ViT/run_pretrain.py", line 223, in main
    writer.write(optimizer_param_groups)
TypeError: write() argument must be str, not list
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 1204106) of binary: /opt/conda/bin/python3
Traceback (most recent call last):
  File "/opt/conda/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/opt/conda/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/launch.py", line 195, in <module>
    main()
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/launch.py", line 191, in main
    launch(args)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/launch.py", line 176, in launch
    run(args)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/run.py", line 753, in run
    elastic_launch(
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 132, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 246, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
run_pretrain.py FAILED
------------------------------------------------------------
Failures:
[1]:
  time      : 2024-01-24_07:58:53
  host      : 5a03a66edd0a
  rank      : 1 (local_rank: 1)
  exitcode  : 1 (pid: 1204107)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-01-24_07:58:53
  host      : 5a03a66edd0a
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 1204106)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
